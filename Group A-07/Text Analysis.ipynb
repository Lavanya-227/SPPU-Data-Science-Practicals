{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74e8beaf-1f2b-4307-9961-8de9d101f900",
   "metadata": {},
   "source": [
    "# GROUP A - 07\n",
    "\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of documents by calculating Term Frequency and Inverse\n",
    "DocumentFrequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742796b8-f193-433f-8342-bf7d8feee30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AI&DS_AIL_18\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AI&DS_AIL_18\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AI&DS_AIL_18\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AI&DS_AIL_18\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#1. Document preprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5df63f9-8cc7-4ecb-9790-bdd52ebff66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856b6a63-179a-4486-a4d2-de15b69f42ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK is a leading platform for building Python programs to work with human language data.', 'It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "tokenize_sentence = sent_tokenize(text)\n",
    "print(tokenize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220e8c7b-c582-4ff1-9d43-4001219479d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', ',', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenize\n",
    "tokenize_word = word_tokenize(text)\n",
    "print(tokenize_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d34cfac1-d680-43dd-9520-e4f6dab6253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.'), ('It', 'PRP'), ('provides', 'VBZ'), ('easy-to-use', 'JJ'), ('interfaces', 'NNS'), ('to', 'TO'), ('over', 'IN'), ('50', 'CD'), ('corpora', 'NNS'), ('and', 'CC'), ('lexical', 'JJ'), ('resources', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('WordNet', 'NNP'), (',', ','), ('along', 'IN'), ('with', 'IN'), ('a', 'DT'), ('suite', 'NN'), ('of', 'IN'), ('text', 'NN'), ('processing', 'NN'), ('libraries', 'NNS'), ('for', 'IN'), ('classification', 'NN'), (',', ','), ('tokenization', 'NN'), (',', ','), ('stemming', 'VBG'), (',', ','), ('tagging', 'VBG'), (',', ','), ('parsing', 'NN'), (',', ','), ('and', 'CC'), ('semantic', 'JJ'), ('reasoning', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "tagging = nltk.pos_tag(tokenize_word)\n",
    "print(tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14f5428-3b46-4a08-b981-135b9daeeb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.', 'provides', 'easy-to-use', 'interfaces', '50', 'corpora', 'lexical', 'resources', ',', 'WordNet', ',', 'along', 'suite', 'text', 'processing', 'libraries', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'semantic', 'reasoning', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "s_words = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in tokenize_word:\n",
    "    if i.lower() not in stop_words:\n",
    "        s_words.append(i)\n",
    "print(s_words)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e13135-e853-4959-9580-9eec565e377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'human', 'languag', 'data', '.', 'it', 'provid', 'easy-to-us', 'interfac', 'to', 'over', '50', 'corpora', 'and', 'lexic', 'resourc', ',', 'such', 'as', 'wordnet', ',', 'along', 'with', 'a', 'suit', 'of', 'text', 'process', 'librari', 'for', 'classif', ',', 'token', ',', 'stem', ',', 'tag', ',', 'pars', ',', 'and', 'semant', 'reason', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "stem = []\n",
    "for i in tokenize_word:\n",
    "    ps = PorterStemmer()\n",
    "    word_stem = ps.stem(i)\n",
    "    stem.append(word_stem)\n",
    "\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef3f73c5-2bb0-4d92-ad8d-142f7b9f2f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmetization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bab2a82-8195-490a-8e65-e99774652a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'program', 'work', 'human', 'language', 'data', '.', 'provides', 'easy-to-use', 'interface', '50', 'corpus', 'lexical', 'resource', ',', 'WordNet', ',', 'along', 'suite', 'text', 'processing', 'library', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'semantic', 'reasoning', '.']\n"
     ]
    }
   ],
   "source": [
    "lemm = []\n",
    "\n",
    "for i in s_words:\n",
    "    wl = WordNetLemmatizer()\n",
    "    lemma = wl.lemmatize(i)\n",
    "    lemm.append(lemma)\n",
    "print(lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7401b18b-a17c-4841-abee-721ce2f3538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Calculating Term Frequency and Inverse DocumentFrequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "468ada5a-2e50-4dd0-9d8e-940e732f4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_document = \" \".join(lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "644694ec-2966-48e1-b6f0-21373764d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "171ef22e-f5e4-4da4-9d69-d5199096a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = vectorizer.fit_transform([preprocessed_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93ac82ac-0c68-4b24-8ceb-ffcb50ba2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de341e-7b24-4c4c-b46e-dc1663a6e7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
